name: NetworkPolicy tests

on:
  push:
    branches: [ "main" ]

  pull_request:
    branches: [ "main", "release-*" ]

defaults:
  run:
    shell: bash
    working-directory: ./src/knative.dev/serving

jobs:
  e2e-connectivity:
    name: e2e connectivity
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        k8s-version:
        - v1.22.0

        test-suite:
        - ./test/conformance/api/v1

        include:
        - k8s-version: v1.22.0
          kind-version: v0.11.1
          kingress: istio
          kind-image-sha: sha256:f97edf7f7ed53c57762b24f90a34fad101386c5bd4d93baeb45449557148c717
          cluster-suffix: c${{ github.run_id }}.local
    env:
      GOPATH: ${{ github.workspace }}
      GO111MODULE: on
      GOFLAGS: -tags=nostackdriver
      # https://github.com/google/go-containerregistry/pull/125 allows insecure registry for
      # '*.local' hostnames. This works both for `ko` and our own tag-to-digest resolution logic,
      # thus allowing us to test without bypassing tag-to-digest resolution.
      REGISTRY_NAME: registry.local
      REGISTRY_PORT: 5000
      KO_DOCKER_REPO: registry.local:5000/knative
    steps:
    - name: Set up Go 1.16.x
      uses: actions/setup-go@v2
      with:
        go-version: 1.16.x

    - name: Install Dependencies
      working-directory: ./
      run: |
        echo '::group:: install ko'
        curl -L https://github.com/google/ko/releases/download/v0.6.0/ko_0.6.0_Linux_x86_64.tar.gz | tar xzf - ko
        chmod +x ./ko
        sudo mv ko /usr/local/bin
        echo '::endgroup::'
    - name: Check out code onto GOPATH
      uses: actions/checkout@v2
      with:
        path: ./src/knative.dev/serving

    - name: Install KinD
      run: |
        set -x
        # Disable swap otherwise memory enforcement doesn't work
        # See: https://kubernetes.slack.com/archives/CEKK1KTN2/p1600009955324200
        sudo swapoff -a
        sudo rm -f /swapfile
        # Use in-memory storage to avoid etcd server timeouts.
        # https://kubernetes.slack.com/archives/CEKK1KTN2/p1615134111016300
        # https://github.com/kubernetes-sigs/kind/issues/845
        sudo mkdir -p /tmp/etcd
        sudo mount -t tmpfs tmpfs /tmp/etcd
        curl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/${{ matrix.kind-version }}/kind-$(uname)-amd64
        chmod +x ./kind
        sudo mv kind /usr/local/bin
    - name: Configure KinD Cluster
      working-directory: ./src/knative.dev/serving
      run: |
        set -x
        # KinD configuration.
        cat > kind.yaml <<EOF
        apiVersion: kind.x-k8s.io/v1alpha4
        kind: Cluster
        # Configure registry for KinD.
        containerdConfigPatches:
        - |-
          [plugins."io.containerd.grpc.v1.cri".registry.mirrors."$REGISTRY_NAME:$REGISTRY_PORT"]
            endpoint = ["http://$REGISTRY_NAME:$REGISTRY_PORT"]
        # This is needed in order to support projected volumes with service account tokens.
        # See: https://kubernetes.slack.com/archives/CEKK1KTN2/p1600268272383600
        kubeadmConfigPatches:
          - |
            apiVersion: kubeadm.k8s.io/v1beta2
            kind: ClusterConfiguration
            metadata:
              name: config
            apiServer:
              extraArgs:
                "service-account-issuer": "kubernetes.default.svc"
                "service-account-signing-key-file": "/etc/kubernetes/pki/sa.key"
            networking:
              dnsDomain: "${{ matrix.cluster-suffix }}"
        nodes:
        - role: control-plane
          image: kindest/node:${{ matrix.k8s-version }}@${{ matrix.kind-image-sha }}
          extraMounts:
          - containerPath: /var/lib/etcd
            hostPath: /tmp/etcd
        - role: worker
          image: kindest/node:${{ matrix.k8s-version }}@${{ matrix.kind-image-sha }}
        EOF
    - name: Add Workers to KinD Cluster (Istio)
      working-directory: ./src/knative.dev/serving
      if: matrix.kingress == 'istio'
      run: |
        set -x
        cat >> kind.yaml <<EOF
        - role: worker
          image: kindest/node:${{ matrix.k8s-version }}@${{ matrix.kind-image-sha }}
        - role: worker
          image: kindest/node:${{ matrix.k8s-version }}@${{ matrix.kind-image-sha }}
        - role: worker
          image: kindest/node:${{ matrix.k8s-version }}@${{ matrix.kind-image-sha }}
        EOF
    
    - name: Create KinD Cluster
      working-directory: ./src/knative.dev/serving
      run: |
        set -x
        file /usr/local/bin/kind
        kind create cluster --config kind.yaml
    - name: Setup NetworkPolicy CNI
      run: |
        kubectl apply -f https://docs.projectcalico.org/v3.20/manifests/calico.yaml
    
    - name: Setup local registry
      run: |
        # Run a registry.
        docker run -d --restart=always \
          -p $REGISTRY_PORT:$REGISTRY_PORT --name $REGISTRY_NAME registry:2
        # Connect the registry to the KinD network.
        docker network connect "kind" $REGISTRY_NAME
        # Make the $REGISTRY_NAME -> 127.0.0.1, to tell `ko` to publish to
        # local reigstry, even when pushing $REGISTRY_NAME:$REGISTRY_PORT/some/image
        sudo echo "127.0.0.1 $REGISTRY_NAME" | sudo tee -a /etc/hosts
    
    - name: Install Serving & Ingress
      working-directory: ./src/knative.dev/serving
      run: |
        source ./test/e2e-common.sh
        KIND=1
        INGRESS_CLASS="${{ matrix.kingress }}.ingress.networking.knative.dev"
        CLUSTER_DOMAIN="${{ matrix.cluster-suffix }}"
        knative_setup
        test_setup
        echo "INGRESS_CLASS=$INGRESS_CLASS" >> $GITHUB_ENV
        echo "CLUSTER_DOMAIN=$CLUSTER_DOMAIN" >> $GITHUB_ENV
        echo "SYSTEM_NAMESPACE=$SYSTEM_NAMESPACE" >> $GITHUB_ENV
        echo "GATEWAY_OVERRIDE=$GATEWAY_OVERRIDE" >> $GITHUB_ENV
        echo "GATEWAY_NAMESPACE_OVERRIDE=$GATEWAY_NAMESPACE_OVERRIDE" >> $GITHUB_ENV
    
    - name: Apply NetworkPolicy
      working-directory: ./src/knative.dev/serving
      run: |
        set -x
        cat >> netpolicy.yaml <<EOF
        apiVersion: networking.k8s.io/v1
        kind: NetworkPolicy
        metadata:
          name: default-deny
          namespace: $SYSTEM_NAMESPACE
        spec:
          podSelector:
            matchExpressions:
            - key: app
              operator: In
              values: [autoscaler, autoscaler-hpa]
          policyTypes:
          - Ingress
          ingress:
          - from:
            - namespaceSelector:
                matchLabels:
                  # TODO: use this in preference to the projectcalico.org/name label after minVersion >= k8s 1.21.1
                  kubernetes.io/metadata.name: $SYSTEM_NAMESPACE
                  # When using tests against k8s versions lower than k8s 1.21.1 use the following label instead
                  # projectcalico.org/name: $SYSTEM_NAMESPACE # calico specific
        EOF
        kubectl apply -f netpolicy.yaml
    
    - name: Check that autoscaler pods are unreachable
      run: |
        set -x
        AUTOSCALER_IP=$(kubectl -n $SYSTEM_NAMESPACE get po -l 'app=autoscaler' -o jsonpath='{.items[*].status.podIPs[*].ip}')
        # ping from inside the installation namespace should succeed
        kubectl run pinger --rm -i --image=busybox --namespace=$SYSTEM_NAMESPACE --restart=Never -- ping -c 8 $AUTOSCALER_IP
        PING_FAILED=false
        function catch_ping_err {
          if $PING_FAILED; then
            echo "pinging autoscaler failed as expected"
          else
            echo "pinging autoscaler should have failed but succeeded"
            exit 1
          fi
        }
        trap catch_ping_err ERR
        # ping from outside the installation namespace should fail
        (kubectl run pinger --rm -i --image=busybox --restart=Never -- ping -c 8 $AUTOSCALER_IP || PING_FAILED=true)

    - name: Run Test
      working-directory: ./src/knative.dev/serving
      run: |
        set -x

        # Exclude the control-plane node, which doesn't seem to expose the nodeport service.
        IPS=( $(kubectl get nodes -lkubernetes.io/hostname!=kind-control-plane -ojsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}') )
        # Run the tests tagged as e2e on the KinD cluster.
        go test -race -count=1 -timeout=20m -tags=e2e ${{ matrix.test-suite }} \
           --ingressendpoint="${IPS[0]}" \
           ${{ matrix.test-flags }}